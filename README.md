# VQA_for_ML_classes
Visual Question Answering (VQA) System for Enhanced Understanding of Machine Learning Classes

This study undertakes twin objectives in the field of Visual Question Answering (VQA): the construction of a comprehensive dataset based on Yeshiva University's machine learning course, and the evaluation of several advanced VQA models. The dataset was painstakingly assembled, including the collection of images and transcripts and the creation of question-answer pairs, despite challenges posed by the complicated nature of the image collection and the variable quality of the source audio material. The core of this project was the integration and assessment of five distinct VQA models: LLaVA 1.5, Salesforce's BLIP, a custom GPT2-based model with Vision Transformer (DeiT), Pix2Struct, and the newly included BLIP-2-FLAN-T5-XL. These models form the basis of a sophisticated mul-timodal VQA system designed to go beyond traditional educational tools. Each model has been rigorously evaluated using key metrics such as cosine similarity, BLEU score, and ROUGE score to ensure a thorough analysis of their ability to process and interpret complex educational content. The study's outcomes, encompassing the dataset creation and the comparative model analysis, offer significant contributions to the VQA domain. The findings illuminate the intricacies and prospects of developing systems capable of intricate understanding and response generation in visually situated language scenarios. This work paves the way for future advancements in artificial intelligence and machine learning, particularly in enhancing interactive and intelligent visual data interpretation.

LINK ON THE PAPER: 
https://www.researchgate.net/publication/377299493_Visual_Question_Answering_VQA_System_for_Enhanced_Understanding_of_Machine_Learning_Classes
